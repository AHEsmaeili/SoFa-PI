{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba79bf3-17f5-4360-a12c-71e7fc8d8091",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d8d0f-3efb-44c7-baf0-d025f4c9c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import sys\n",
    "import shelve\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import ptitprince as pt\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "from scipy import signal, stats, interpolate\n",
    "from scipy.io import loadmat\n",
    "from scipy.fft import *\n",
    "from sklearn import preprocessing\n",
    "import sklearn as skl\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89ee97-f3e3-47f0-b86c-7d0d2e31c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeStringArray(stringArray):\n",
    "    outputVector = []\n",
    "    for i in range(len(stringArray[0])):\n",
    "        outputVector.append(stringArray[0][i]+stringArray[1][i]+stringArray[2][i])\n",
    "    return  np.array(outputVector)\n",
    "\n",
    "def morlet_transform(array, sampling_freq, scale_max):\n",
    "    scales = np.arange(1, scale_max)  ##Return numbers spaced evenly on a log scale (a geometric progression).\n",
    "    array = [(i - np.min(array)) / (np.max(array) - np.min(array)) for i in array]  # NORMALIZE - important\n",
    "    # array = skl.preprocessing.MinMaxScaler().fit_transform(array.reshape(1,-1))\n",
    "    # wavelet transform\n",
    "    coef, freqs = pywt.cwt(array, scales, \"cmor1.5-1.0\", sampling_period = 1/sampling_freq)\n",
    "    power = abs(coef)\n",
    "    average_power = np.mean(power, axis=1)\n",
    "    return coef, freqs, power, average_power, scales\n",
    "\n",
    "\n",
    "def surrogate(df, col_index, sampling_freq, scale_max, n_surrogate=10):\n",
    "    \"\"\"Randomly shuffled datapoint-datasets analysis to contrast peaks in ori data (if any)\"\"\"\n",
    "\n",
    "    arr2shuffle = copy.deepcopy(df)\n",
    "    arr2shuffle = list(arr2shuffle.iloc[:, col_index])\n",
    "    shuffled = []\n",
    "    for i in range(n_surrogate + 1):\n",
    "        shuffled.append(random.sample(arr2shuffle, k=len(arr2shuffle)))\n",
    "\n",
    "    coefs = []\n",
    "    freqs = []\n",
    "    power = []\n",
    "    average_power = []\n",
    "    for arr in shuffled:\n",
    "        coefs.append(morlet_transform(array=arr, sampling_freq=sampling_freq, scale_max=scale_max)[0])\n",
    "        freqs.append(morlet_transform(array=arr, sampling_freq=sampling_freq, scale_max=scale_max)[1])\n",
    "        power.append(morlet_transform(array=arr, sampling_freq=sampling_freq, scale_max=scale_max)[2])\n",
    "        average_power.append(morlet_transform(array=arr, sampling_freq=sampling_freq, scale_max=scale_max)[3])\n",
    "        ssd = np.std(average_power, axis=0)\n",
    "    coefs, freqs, power, average_power = [np.mean(i, axis=0) for i in [coefs, freqs, power, average_power]]\n",
    "\n",
    "    return coefs, freqs, power, average_power, ssd, shuffled\n",
    "\n",
    "def processEventFile(eventDict):\n",
    "    eventTimes = eventDict['time']\n",
    "    eventDurs = eventTimes[:,-1] - eventTimes[:,0]\n",
    "    eventMask = eventDurs <= 3\n",
    "\n",
    "    maskedEventDict = {}\n",
    "    maskedEventDict['time'] = eventDict['time'][eventMask,:]\n",
    "    maskedEventDict['cond'] = eventDict['cond'][eventMask]\n",
    "    maskedEventDict['sar'] = eventDict['sar'][eventMask,:]\n",
    "    \n",
    "    return maskedEventDict, eventMask\n",
    "\n",
    "def find_nearest(array, values):\n",
    "    # make sure array is a numpy array\n",
    "    array = np.array(array)\n",
    "\n",
    "    # get insert positions\n",
    "    idxs = np.searchsorted(array, values, side=\"left\")\n",
    "    \n",
    "    # find indexes where previous index is closer\n",
    "    prev_idx_is_less = ((idxs == len(array))|(np.fabs(values - array[np.maximum(idxs-1, 0)]) < np.fabs(values - array[np.minimum(idxs, len(array)-1)])))\n",
    "    idxs[prev_idx_is_less] -= 1\n",
    "    \n",
    "    return array[idxs], idxs\n",
    "\n",
    "\n",
    "def annotateTFPlot(axe, eventDict, eventFocus = None, takeAllEvents = True, eventConds = None, onlyTicks = False):\n",
    "    \n",
    "    eventLabels = ['D-On', 'T-L', 'S-On', 'S-Off', 'M-On', 'M-Off', 'FB']\n",
    "    \n",
    "    if takeAllEvents:\n",
    "        eventTimes = eventDict['time']\n",
    "    else:\n",
    "        eventTimes = eventDict['time'][eventConds,:]\n",
    "    \n",
    "            \n",
    "    meanET = np.mean(eventTimes, 0)\n",
    "    meanET -= meanET[0]\n",
    "    meanET -= meanET[eventFocus]\n",
    "\n",
    "    if onlyTicks:\n",
    "        return meanET\n",
    "    \n",
    "    else:    \n",
    "        [ymin, ymax] = axe.get_ylim()\n",
    "        \n",
    "        axe.vlines(meanET, ymin = ymin, ymax = ymax, ls = 'dashed', colors = 'seashell')\n",
    "        \n",
    "        for lInd, label in enumerate(eventLabels):\n",
    "            if lInd == 1:\n",
    "                continue\n",
    "            elif lInd == 3:\n",
    "                xoffset = -0.1\n",
    "            else:\n",
    "                xoffset = 0\n",
    "            axe.text(x = meanET[lInd] + xoffset, y = ymax+1, s = label, c = 'k', rotation = 45)\n",
    "\n",
    "\n",
    "def genDataDicts(fileLoc):\n",
    "    \n",
    "    folders = os.listdir(fileLoc)\n",
    "    \n",
    "    lfpDict = {}\n",
    "    eventDict = {}\n",
    "    prosEventDict = {}\n",
    "    \n",
    "    for folder in folders:\n",
    "        \n",
    "        folderLoc = fileLoc + '/' + folder\n",
    "        files = os.listdir(folderLoc)\n",
    "    \n",
    "        if (folder + \"_LFP.mat\" in files) & ('evt.mat' in files) & (len(folder) == 8):\n",
    "\n",
    "            data = loadmat(folderLoc + '/' + folder + \"_LFP.mat\", squeeze_me = True, simplify_cells = True)\n",
    "            eDict = loadmat(folderLoc + '/' + \"evt.mat\", squeeze_me = True, simplify_cells = True)\n",
    "            \n",
    "            lfpDict[folder] = {'LFP': data['LFP'] ,'sr' : data['LFP_SR'],\n",
    "                            'timestamps' : data['timestamps'],\n",
    "                            'blocs' : transposeStringArray(data['blocs']),\n",
    "                            'blocTimes' : data['blocRanges']}\n",
    "            \n",
    "            eventDict[folder] = eDict['evt']        \n",
    "            eDict2, eMask = processEventFile(eDict['evt'])\n",
    "            prosEventDict[folder] = {'processedEvents': eDict2, 'eMask': eMask}\n",
    "            \n",
    "        \n",
    "    return lfpDict, eventDict, prosEventDict\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32,\n",
    "                              np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def genDataDicts_Spike(fileLoc):\n",
    "    \n",
    "    folders = os.listdir(fileLoc)\n",
    "    \n",
    "    neuronDict = {}\n",
    "    eventDict = {}\n",
    "    prosEvents = {}\n",
    "    prosEventDict = {}\n",
    "    \n",
    "    for folder in folders:\n",
    "        \n",
    "        folderLoc = fileLoc + '/' + folder\n",
    "        files = os.listdir(folderLoc)\n",
    "\n",
    "        \n",
    "        # print(folder)\n",
    "        \n",
    "        if ('spk_lfp_evt_eog.mat' in files) & ('evt.mat' in files) & (len(folder) == 8):\n",
    "        \n",
    "            matSpikes = loadmat(folderLoc + '/' + 'spk_lfp_evt_eog.mat', simplify_cells = True)\n",
    "            matEvents = loadmat(folderLoc + '/' + 'evt.mat', simplify_cells = True)\n",
    "\n",
    "\n",
    "            if type(matSpikes['nexFile']['neurons']) == list:\n",
    "                \n",
    "                for neuron in matSpikes['nexFile']['neurons']:\n",
    "                \n",
    "                    nID = neuron['name']\n",
    "                    nID = nID.replace('Elec_', 'E')\n",
    "                    nID = nID.replace('_Neuron_', 'N')\n",
    "                    rDate = datetime.datetime.strptime(folder, '%d%m%Y').strftime('%d%m%y')\n",
    "                    neuronDict[rDate + nID] = {'timestamps': neuron['timestamps'], 'date': rDate,\n",
    "                                            'Electrode': nID[-3], 'Number': nID[-1],\n",
    "                                            'xPos': neuron['xPos'], 'yPos': neuron['yPos']}\n",
    "            \n",
    "            elif type(matSpikes['nexFile']['neurons']) == dict:\n",
    "                \n",
    "                neuron = matSpikes['nexFile']['neurons']\n",
    "                nID = neuron['name']\n",
    "                nID = nID.replace('Elec_', 'E')\n",
    "                nID = nID.replace('_Neuron_', 'N')\n",
    "                rDate = datetime.datetime.strptime(folder, '%d%m%Y').strftime('%d%m%y')\n",
    "                neuronDict[rDate + nID] = {'timestamps': neuron['timestamps'], 'date': rDate,\n",
    "                                            'Electrode': nID[-3], 'Number': nID[-1],\n",
    "                                            'xPos': neuron['xPos'], 'yPos': neuron['yPos']}\n",
    "                \n",
    "            \n",
    "    return neuronDict\n",
    "\n",
    "def formatData(data, time, timeLocks, binSize, maxLead, maxLag, sr=781.25, isComplex=False):\n",
    "    \n",
    "    if len(data) != len(time):\n",
    "        raise Exception('Data and time must have equal length')\n",
    "\n",
    "    num_trials = len(timeLocks)\n",
    "    \n",
    "    rangeInds = np.round(np.array([maxLead*sr, maxLag*sr], dtype = np.int64))\n",
    "    \n",
    "    nearestTimes, nearestInds = find_nearest(time, timeLocks)\n",
    "    beginInds = nearestInds - rangeInds[0]\n",
    "    endInds = nearestInds + rangeInds[-1]\n",
    "    \n",
    "    fData = np.zeros((np.abs(rangeInds).sum(), data.shape[1], num_trials))\n",
    "\n",
    "    if isComplex:\n",
    "        fData = fData.astype(complex)\n",
    "    \n",
    "    for trialInd in range(num_trials):\n",
    "        \n",
    "        trial_data = data[beginInds[trialInd] : endInds[trialInd],:]\n",
    "        trial_length = trial_data.shape[0]\n",
    "        \n",
    "        if trial_length >= fData.shape[0]:\n",
    "            fData[:, :, trialInd] = trial_data\n",
    "        else:\n",
    "            fData[:trial_length, :, trialInd] = trial_data\n",
    "    \n",
    "    return fData\n",
    "\n",
    "def formatData_Spike(data, time, timeLocks, binSize, maxLead, maxLag, method):\n",
    "    \n",
    "    if len(data) != len(time):\n",
    "        raise Exception('Data and time must have equal length')\n",
    "\n",
    "    binBounds = np.linspace(-np.round(maxLead/binSize, 1),np.round(maxLag/binSize, 1), int((maxLead+maxLag)/binSize)+1)*binSize\n",
    "    timeBounds = np.array(list(zip(binBounds[0:-1],binBounds[1:])))\n",
    "    \n",
    "    nTrials = len(timeLocks)\n",
    "    nBins = len(timeBounds)\n",
    "\n",
    "    fData = np.zeros((nBins, nTrials))\n",
    "\n",
    "    for iTrial in range(nTrials):\n",
    "\n",
    "        for iBin in range(nBins):\n",
    "            if method == 'count':\n",
    "                fData[iBin, iTrial] = np.sum(data[(time >= (timeLocks[iTrial] + timeBounds[iBin][0])) & (time < (timeLocks[iTrial] + timeBounds[iBin][1]))])\n",
    "            elif method == 'rate':\n",
    "                spkCount = np.sum(data[(time >= (timeLocks[iTrial] + timeBounds[iBin][0])) & (time < (timeLocks[iTrial] + timeBounds[iBin][1]))])     \n",
    "                fData[iBin, iTrial] = spkCount/np.diff(timeBounds[iBin,:])\n",
    "                \n",
    "    return fData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71315d7-5b26-422a-a87d-af770d5cffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_to_rate(spiketimes, nbins = 100, remove_tails = False, axis = -1):\n",
    "\n",
    "    spiketimes = spiketimes[np.array(spiketimes)!=None]\n",
    "\n",
    "    binned_spikes, _ = np.histogram(spiketimes, bins = nbins, range = (0,1000))\n",
    "\n",
    "    binned_fr = smooth_rates(binned_spikes, nbins = nbins, remove_tails = remove_tails, axis = axis)\n",
    "    \n",
    "    return binned_fr\n",
    "\n",
    "def smooth_rates(firing_rate, nbins = 100, remove_tails = False, axis = -1, order = 5, lp_savgol = 3, lp_filtfilt = 2):\n",
    "    \n",
    "    nneigh = 70\n",
    "    \n",
    "    if remove_tails:\n",
    "        lowpass = signal.butter(order, lp_savgol, 'lp', fs=nbins, output='sos')\n",
    "        firing_rate = signal.savgol_filter(firing_rate, nneigh, order, mode = 'mirror', axis = axis)\n",
    "    else:\n",
    "        lowpass = signal.butter(order, lp_filtfilt, 'lp', fs=nbins, output='sos')\n",
    "\n",
    "    firing_rate = signal.sosfiltfilt(lowpass, firing_rate, axis = axis)\n",
    "    \n",
    "    return firing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdb855-0dac-4177-924a-d117beee776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster(raster, ax, axis = -1, offset = 0, marker = '.', color = 'b', alpha = 1, markersize = 1):\n",
    "\n",
    "    raster = np.array(raster)\n",
    "    raster[raster > 1] = 1    \n",
    "    \n",
    "    for rInd in range(raster.shape[axis]):\n",
    "        \n",
    "        t_spikes = raster[:,rInd]\n",
    "        sinds = np.where(t_spikes != 0)[0]\n",
    "        ax.scatter(sinds, t_spikes[sinds]*rInd + offset, marker = marker, color = color, alpha = alpha, s = markersize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b884e08-8b1a-460b-ac78-b540543546eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_ERPs(fig, ax, data, color='k', lw=1, alpha=0.5):\n",
    "\n",
    "    max_lead = data.attrs['MaxLead']*1000\n",
    "    max_lag = data.attrs['MaxLag']*1000\n",
    "    \n",
    "    time_vec = np.linspace(-max_lead, max_lag, data.shape[0])    \n",
    "\n",
    "    ax.plot(time_vec, data.to_numpy().reshape(time_vec.shape[0], -1), c=color, lw=lw, alpha=alpha)\n",
    "\n",
    "    ymin, ymax = data.min().round(1), data.max().round(1)\n",
    "    \n",
    "    ax.set_xticks([-max_lead, max_lag])    \n",
    "    ax.set_xticklabels([str(int(-max_lead)), 'FB'])\n",
    "    ax.set_ylabel(r'Amplitude (mV)')\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_aspect('auto', adjustable='box')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40199b87-b26d-4b5f-81dd-8b7367c475d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colors(n):\n",
    "    \"\"\"\n",
    "    Generates a list of n distinct colors.\n",
    "    \n",
    "    Parameters:\n",
    "    n (int): The number of colors to generate.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of n color codes in hexadecimal format.\n",
    "    \"\"\"\n",
    "    colors = plt.cm.get_cmap('hsv', n)\n",
    "    return [colors(i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38771242-7399-43a4-98db-99107fafcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_erp_maxs(fig, ax, data, linewidth=1, point_size=5, labelpad=10, tickpad=10, saturation=0.8, alpha=1, width_viol=0.4, width_box=0.4, bw=0.4,\n",
    "                  pointplot=False, orient='v', linecolor='darkslategray', move=0, offset=0, legend=False, hide_ticks=True):\n",
    "\n",
    "    palette = {'Presence': 'dodgerblue', 'Absence': 'crimson'}\n",
    "    \n",
    "    significanceComparisons=[('Presence','Absence')]\n",
    "    configuration = {'test':'Mann-Whitney',  'text_format':'star', 'loc':'outside', 'line_width':linewidth}\n",
    "\n",
    "    if orient == 'h':\n",
    "        x, y = 'value', 'condition'\n",
    "    else:\n",
    "        x, y = 'condition', 'value'\n",
    "    \n",
    "    fig_args = {'x': x, 'y': y, 'data': data, 'dodge': True, 'palette':palette, 'linecolor': linecolor,\n",
    "                'point_size':point_size, 'linewidth':linewidth, 'box_linewidth': linewidth, 'saturation':saturation, 'alpha': alpha,\n",
    "                'width_viol':width_viol, 'width_box':width_box, 'bw':bw}\n",
    "    \n",
    "    rainclouds = pt.RainCloud(ax=ax, orient=orient, **fig_args, cut=0, pointplot=pointplot, box_fliersize=0, box_whiskerprops=dict(linewidth=linewidth))\n",
    "\n",
    "    if legend:\n",
    "        handles, labels = rainclouds.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels=['Presence', 'Absence'], frameon=False)  # Adjust labels as needed\n",
    "    \n",
    "    annotator = Annotator(ax=ax, pairs=significanceComparisons, **fig_args, plot='boxplot', verbose=False)\n",
    "    annotator.configure(**configuration).apply_test().annotate()\n",
    "\n",
    "    if orient == 'h':\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('Peak ERP amplitude', labelpad=labelpad)\n",
    "        ax.tick_params(axis='y', length=0, pad=tickpad)\n",
    "        ax.tick_params(axis='x',)\n",
    "        if hide_ticks:\n",
    "            ax.set_yticks([])\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Peak ERP amplitude', labelpad=labelpad)\n",
    "        ax.tick_params(axis='x', length=0, pad=tickpad)\n",
    "        ax.tick_params(axis='y')\n",
    "        if hide_ticks:\n",
    "            ax.set_xticks([])\n",
    "    \n",
    "    ax.patch.set_alpha(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409acbb9-b968-47f4-85a3-abdf3f46d7de",
   "metadata": {},
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815b46a-aff1-45be-aa5b-be350bdaf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_preprocess_dir = ''\n",
    "\n",
    "lfpDict, eventDict, prosEventDict = genDataDicts('')\n",
    "neurDict = genDataDicts_Spike('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8575e70-3725-4fbb-b5df-6ad584598de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sessions = list(lfpDict.keys())\n",
    "nSessions = len(Sessions)\n",
    "sampling_rate = lfpDict['01042014']['sr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe6757-19a9-400b-a725-fc74e3563cac",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17d3b7-6fca-4fda-9f84-5632f012a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenLFPs = [len(v['timestamps']) for k,v in lfpDict.items()]\n",
    "lenTrials = [len(v['processedEvents']['cond']) for k,v in prosEventDict.items()]\n",
    "maxTimestamps = max(lenLFPs)\n",
    "maxTrialLens = max(lenTrials)    \n",
    "\n",
    "centerEvent = -1\n",
    "\n",
    "session = Sessions[0]\n",
    "LFP = lfpDict[session]['LFP']\n",
    "timestamps = lfpDict[session]['timestamps']        \n",
    "sessionEventDict = prosEventDict[session]['processedEvents']\n",
    "timeLocks = sessionEventDict['time'][:,centerEvent]\n",
    "\n",
    "aggEvtArray = np.empty((maxTrialLens, 3, nSessions), dtype = np.float64)\n",
    "aggBehArray = np.empty((maxTrialLens, 3, nSessions), dtype = np.float64)\n",
    "aggCndArray = np.empty((maxTrialLens, nSessions), dtype = np.float64)\n",
    "    \n",
    "for sInd, session in enumerate(prosEventDict):\n",
    "\n",
    "    sessionEventDict = prosEventDict[session]['processedEvents']\n",
    "\n",
    "    session_time = sessionEventDict['time']\n",
    "    session_events = sessionEventDict['sar']\n",
    "    session_conds = sessionEventDict['cond']\n",
    "\n",
    "    num_trials = session_events.shape[0]\n",
    "    n_sess_evts = session_events.shape[1]\n",
    "\n",
    "    if n_sess_evts == 7:\n",
    "        session_events = session_events[:, [2,4,6]]\n",
    "        session_events[:,1] -= session_events[:,1].min()\n",
    "    \n",
    "    session_time = session_time[:, [2,4,6]]\n",
    "    \n",
    "    aggEvtArray[:num_trials, :, sInd] = session_time\n",
    "    aggEvtArray[num_trials:, :, sInd] = np.nan\n",
    "    aggBehArray[:num_trials, :, sInd] = session_events\n",
    "    aggBehArray[num_trials:, :, sInd] = np.nan\n",
    "    aggCndArray[:num_trials, sInd] = session_conds\n",
    "    aggCndArray[num_trials:, sInd] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b336f-b341-4ff9-a1ae-778e21df7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventDims = [\"trial\", \"event\", \"session\"]\n",
    "eventCoordDict = {\"session\": Sessions, \"event\": ['cue', 'movement', 'feedback']}\n",
    "\n",
    "xr.DataArray(aggEvtArray, dims = eventDims, coords = eventCoordDict).to_netcdf(parent_preprocess_dir + 'Demolliens_aggEvents_All.nc')\n",
    "xr.DataArray(aggBehArray, dims = eventDims, coords = eventCoordDict).to_netcdf(parent_preprocess_dir + 'Demolliens_aggBehaviors_All.nc')\n",
    "xr.DataArray(aggCndArray, dims = [\"trial\", \"session\"], coords = {\"session\": Sessions}).to_netcdf(parent_preprocess_dir + 'Demolliens_aggConds_All.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681248b6-1791-472b-b69f-998be647ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggEvtArray = xr.load_dataarray(parent_preprocess_dir + 'Demolliens_aggEvents_All.nc')\n",
    "aggBehArray = xr.load_dataarray(parent_preprocess_dir + 'Demolliens_aggBehaviors_All.nc')\n",
    "aggCndArray = xr.load_dataarray(parent_preprocess_dir + 'Demolliens_aggConds_All.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0747d-690e-4dc3-a500-8fc5b68a63d0",
   "metadata": {},
   "source": [
    "# Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26f612-acd9-4a4a-9d08-fdd18994829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLead = 0.5\n",
    "maxLag = 0.5\n",
    "centerEvent = -1\n",
    "binsize = 0.005\n",
    "\n",
    "lenList = []\n",
    "typeList = []\n",
    "for name, neuron in neurDict.items():\n",
    "    ts = neuron['timestamps']\n",
    "    typeList.append(type(ts))\n",
    "    if type(ts) == np.ndarray:\n",
    "        lenList.append(len(neuron['timestamps']))\n",
    "    else:\n",
    "        lenList.append(1)\n",
    "lenList = np.array(lenList)\n",
    "\n",
    "neuronNames = np.array(list(neurDict.keys()))\n",
    "goodNeurons = neuronNames[np.where(lenList >= 1000)[0]]\n",
    "neurDict = {k:v for k,v in neurDict.items()if (k in goodNeurons)\n",
    "            and (datetime.datetime.strptime(v['date'], '%d%m%y').strftime('%d%m%Y') in Sessions)}\n",
    "neuronNames = np.array(list(neurDict.keys()))\n",
    "\n",
    "neuron = neurDict['010414E1N1']\n",
    "session = datetime.datetime.strptime(neuron['date'], '%d%m%y').strftime('%d%m%Y')\n",
    "timeLocks = prosEventDict[session]['processedEvents']['time'][:,centerEvent]\n",
    "timestamps = neuron['timestamps']\n",
    "\n",
    "firstArray = formatData_Spike(np.ones((len(timestamps))), timestamps,\n",
    "                                  timeLocks, binsize, maxLead, maxLag, 'count')\n",
    "\n",
    "neuron_properties = np.zeros((len(neuronNames), 5), dtype=object)\n",
    "neuron_array = np.zeros((firstArray.shape[0], maxTrialLens, len(list(neurDict.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bf7be-05c1-4036-bf5b-7ed72e13fe3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nInd, (n_name, neuron) in enumerate(neurDict.items()):\n",
    "\n",
    "    if nInd%10 == 0:\n",
    "        print(nInd)\n",
    "\n",
    "    timestamps, n_date, e_ind, n_num, x, y = list(neuron.values())\n",
    "    \n",
    "    if e_ind == 'N':\n",
    "        e_ind = '1'\n",
    "    \n",
    "    session = datetime.datetime.strptime(neuron['date'], '%d%m%y').strftime('%d%m%Y')\n",
    "    timeLocks = prosEventDict[session]['processedEvents']['time'][:,centerEvent]\n",
    "    nTrials = len(timeLocks)\n",
    "\n",
    "    neuronTrials = formatData_Spike(np.ones((len(timestamps))), timestamps, timeLocks, binsize, maxLead, maxLag, 'count')\n",
    "\n",
    "    neuron_array[:, :nTrials, nInd] = neuronTrials\n",
    "    neuron_array[:, nTrials:, nInd] = np.nan\n",
    "    neuron_properties[nInd, :] = np.array([session, e_ind, n_num, x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456f816-3a01-4276-8ea7-cbb949af0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "propCoords = {'neuron': neuronNames, 'property': [\"date\", \"electrode\", \"number\", \"x\", \"y\"]}\n",
    "propDims = ['neuron', 'property']\n",
    "neuronDims = [\"time\", \"trial\", \"neuron\"]\n",
    "neuronCoords = {\"neuron\": neuronNames}\n",
    "\n",
    "neuron_array = xr.DataArray(neuron_array, dims=neuronDims, coords=neuronCoords)\n",
    "neuron_properties = xr.DataArray(neuron_properties, dims=propDims, coords=propCoords)\n",
    "neuron_dataset = xr.Dataset({'neuron_array':neuron_array, 'neuron_properties': neuron_properties})\n",
    "neuron_dataset.to_netcdf(parent_preprocess_dir + 'Demolliens_aggNeurons_All.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf27c74-210b-4f65-967c-c4c9cd5903d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_dataset = xr.load_dataset(parent_preprocess_dir + 'Demolliens_aggNeurons_All.nc')\n",
    "neuron_array = neuron_dataset.neuron_array\n",
    "neuron_properties = neuron_dataset.neuron_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1291-287e-4f04-b1d0-d74902508a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_array = xr.DataArray(np.zeros((neuron_array.time.shape[0], 50, 2, neuron_array.neuron.shape[0])), dims=('time', 'trial', 'condition', 'neuron'),\n",
    "        coords=dict(condition=['Presence', 'Absence'], neuron=neuron_array.neuron), attrs={'feedback': 'negative', 'maxLead':maxLead, 'maxLag': maxLag})\n",
    "\n",
    "for n_ind, neuron_name in enumerate(neuron_array.neuron):\n",
    "\n",
    "    if nInd%10 == 0:\n",
    "        print(nInd)\n",
    "    \n",
    "    neuron = neuron_array.sel(neuron=neuron_name)\n",
    "    \n",
    "    neuron_session = neuron_properties.sel(neuron=neuron_name, property='date')\n",
    "    neuron_timestamps = aggEvtArray.sel(session=neuron_session)\n",
    "    neuron_behaviours = aggBehArray.sel(session=neuron_session)\n",
    "    neuron_conditions = aggCndArray.sel(session=neuron_session)\n",
    "    \n",
    "    p_cond = neuron_conditions == 3\n",
    "    a_cond = neuron_conditions == 1\n",
    "    \n",
    "    neg_fbs = neuron_behaviours[:,-1] == 0\n",
    "\n",
    "    p_trials = (p_cond & neg_fbs).to_numpy()\n",
    "    a_trials = (a_cond & neg_fbs).to_numpy()\n",
    "    \n",
    "    spikes_array[:, :p_trials.sum(), 0, n_ind] = neuron[:, p_trials].to_numpy()\n",
    "    spikes_array[:, p_trials.sum():, 0, n_ind] = np.nan\n",
    "    \n",
    "    spikes_array[:, :a_trials.sum(), 1, n_ind] = neuron[:, a_trials].to_numpy()\n",
    "    spikes_array[:, a_trials.sum():, 1, n_ind] = np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3843e-041f-47e6-a9d3-8738b9d02707",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_name = '310314E1N3'\n",
    "a_name = '150414E1N2'\n",
    "\n",
    "fig, axes = plt.subplots(2,2, sharex=True)\n",
    "markersize=10\n",
    "\n",
    "plot_raster(spikes_array.sel(neuron=p_name, condition='Presence'), axes[0,0], color='b', markersize=markersize)\n",
    "plot_raster(spikes_array.sel(neuron=p_name, condition='Absence'), axes[1,0], color='r', markersize=markersize)\n",
    "\n",
    "plot_raster(spikes_array.sel(neuron=a_name, condition='Presence'), axes[0,1], color='b', markersize=markersize)\n",
    "plot_raster(spikes_array.sel(neuron=a_name, condition='Absence'), axes[1,1], color='r', markersize=markersize)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f42b79-c3dd-4e7d-aba9-6f3c699122e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_array.to_netcdf(parent_preprocess_dir + 'Demolliens_nFB_Spikes.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191537e-8882-4409-a03d-eac596516363",
   "metadata": {},
   "source": [
    "# Spikes from previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87044aa9-28bf-48ee-8f91-76a92ccd40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string = 'SPK/'\n",
    "spikesParentDir = ''\n",
    "neuron_dirs = glob.glob(spikesParentDir + '*')\n",
    "\n",
    "neuron_names = np.array([n_dir.split('/')[-1] for n_dir in neuron_dirs])\n",
    "neuronStatDict = pd.read_excel('./Social_Asocial_List_2014_NF.xlsx', sheet_name = None, skipfooter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a728a-2e08-4705-b01f-591dfe19e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_spikes = {'Presence': 'cnd3_FN', 'Absence': 'cnd1_FN'}\n",
    "\n",
    "aggSpikes = np.empty((2, 80, 200, len(neuron_names)),dtype = object)\n",
    "aggSpikes = xr.DataArray(aggSpikes, dims = ['condition', 'trial', 'time', 'neuron'], coords = {'condition': list(cond_spikes.keys()), 'neuron': neuron_names})\n",
    "\n",
    "for nInd, nDir in enumerate(neuron_dirs):\n",
    "    neuron = neuron_names[nInd]\n",
    "    \n",
    "    spikeData = loadmat(nDir + '/' + neuron, squeeze_me = True)    \n",
    "        \n",
    "    for cInd, condition in enumerate(cond_spikes):\n",
    "        tempSpikes = np.array(spikeData[cond_spikes[condition]])\n",
    "        aggSpikes[cInd, :tempSpikes.shape[0], :, nInd] = np.array(spikeData[cond_spikes[condition]])\n",
    "        aggSpikes[cInd, tempSpikes.shape[0]:, :, nInd] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17191601-d1db-4321-822a-20463ec9ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice = np.arange(50,150)\n",
    "spike_array = aggSpikes.sel(time=time_slice)\n",
    "\n",
    "neuron_names = spike_array.coords['neuron'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa82c5-b26e-4f82-ac71-7705d394329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "socialNames = neuronStatDict['Social']['Neuron_Name'].to_numpy()\n",
    "asocialNames = neuronStatDict['Asocial']['Neuron_Name'].to_numpy()\n",
    "\n",
    "sNeurons = [neuron for neuron in neuron_names if (neuron in socialNames) and (neuron not in asocialNames)]\n",
    "aNeurons = [neuron for neuron in neuron_names if (neuron in asocialNames) and (neuron not in socialNames)]\n",
    "\n",
    "sel_neurons = sNeurons + aNeurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be236c80-530d-483c-92c9-d6325041d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rates_emp = aggSpikes.sel(neuron=sel_neurons)\n",
    "time_vec = firing_rates_emp['time']\n",
    "\n",
    "firing_rates_emp = firing_rates_emp.copy().mean('trial')\n",
    "firing_rates_emp = xr.DataArray(smooth_rates(firing_rates_emp.to_numpy(), remove_tails=False, axis=1),\n",
    "                                dims=firing_rates_emp.dims, coords=firing_rates_emp.coords)\n",
    "firing_rates_emp = firing_rates_emp[:, time_slice, :]\n",
    "firing_rates_emp /= firing_rates_emp.max('time').max('condition')\n",
    "firing_rates_emp.to_netcdf(parent_preprocess_dir + 'Demolliens_SPK_rates_norm.nc')\n",
    "firing_rates_emp = aggSpikes.sel(neuron=sel_neurons)\n",
    "\n",
    "spike_array.to_netcdf(parent_preprocess_dir + 'Demolliens_SPKs.nc')                      \n",
    "rate_neurons = spike_array.sel(neuron=sel_neurons).copy().mean('trial').max('time').to_dataframe(name = 'value').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c20172-dfc4-41bb-b621-ec98b1a30e1a",
   "metadata": {},
   "source": [
    "# LFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b18735-ce03-43cf-b5e6-4ee60584acaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sessions = list(lfpDict.keys())\n",
    "nSessions = len(Sessions)\n",
    "\n",
    "maxLead = 0.35\n",
    "maxLag = 0.00\n",
    "binSize = 1\n",
    "centerEvent = -1\n",
    "\n",
    "hp_filt = signal.butter(3, 2, 'hp', fs=sampling_rate, output='sos')\n",
    "bandKeys = ['high_pass']\n",
    "\n",
    "session = Sessions[0]\n",
    "LFP = lfpDict[session]['LFP']\n",
    "timestamps = lfpDict[session]['timestamps']        \n",
    "sessionEventDict = prosEventDict[session]['processedEvents']\n",
    "timeLocks = sessionEventDict['time'][:,centerEvent]\n",
    "\n",
    "filteredBand = signal.sosfiltfilt(hp_filt, LFP, axis=0)\n",
    "firstArray = formatData(data=filteredBand, time=timestamps, timeLocks=timeLocks,\n",
    "                                maxLead=maxLead, maxLag=maxLag, binSize=binSize)\n",
    "nTrialTimesteps = firstArray.shape[0]\n",
    "\n",
    "lfp_array = np.empty((nTrialTimesteps, maxTrialLens, 4, nSessions))\n",
    "\n",
    "bads_list = []\n",
    "\n",
    "color_list = generate_colors(nSessions)\n",
    "\n",
    "for sInd, session in enumerate(lfpDict):\n",
    "\n",
    "    if sInd%10 == 0:\n",
    "        print((sInd, session))\n",
    "\n",
    "    LFP = lfpDict[session]['LFP'].astype(np.float32)\n",
    "    bad_channels = LFP.min(0) > -3000\n",
    "\n",
    "    bad_inds = np.where(bad_channels)[0]\n",
    "\n",
    "    bads_list.append(bad_inds)\n",
    "    if bad_channels.sum() > 0:\n",
    "        for _, ch_ind in enumerate(bad_inds):    \n",
    "            LFP[:, ch_ind] = np.nan\n",
    "        \n",
    "    timestamps = lfpDict[session]['timestamps']        \n",
    "    sessionEventDict = prosEventDict[session]['processedEvents']\n",
    "    timeLocks = sessionEventDict['time'][:,centerEvent]\n",
    "    nTrials = len(timeLocks)\n",
    "    \n",
    "    filteredBand = signal.sosfilt(hp_filt, LFP, axis = 0)\n",
    "\n",
    "    sessTrialLFPs = formatData(data = filteredBand, time = timestamps, timeLocks = timeLocks,\n",
    "                                    maxLead = maxLead, maxLag = maxLag, binSize=binSize)\n",
    "    \n",
    "    lfp_array[:, :nTrials, :, sInd] = sessTrialLFPs.swapaxes(1,2)\n",
    "    lfp_array[:, nTrials:, :, sInd] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a529cad-c9fa-4055-967e-bc686866cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfpDims = [\"time\", \"trial\", \"electrode\", \"session\"]\n",
    "lfpCoordDict = {\"session\": Sessions}\n",
    "lfpAttributes = {'MaxLead': maxLead, 'MaxLag': maxLag, 'CenterEvent': centerEvent}\n",
    "lfp_array = xr.DataArray(lfp_array, coords = lfpCoordDict, dims = lfpDims, attrs = lfpAttributes)\n",
    "lfp_array.to_netcdf(parent_preprocess_dir + 'Demolliens_aggLFP_All.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d044a3-0082-4ce7-89c8-8826ab8748a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_array = xr.load_dataarray(parent_preprocess_dir + 'Demolliens_aggLFP_All.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593a8d1-eafd-41bf-a009-f1bc9d3e4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "time_vec = np.linspace(-maxLead*1000, maxLag*1000, lfp_array.time.shape[0])\n",
    "\n",
    "# for sInd in np.arange(3, num_sessions, 5)+1:\n",
    "for sInd in range(nSessions):\n",
    "    ax.plot(lfp_array.isel(session=sInd).mean('trial'));\n",
    "    fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e5906-532f-42b5-a39c-564d232f9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionQuality = pd.read_excel('', converters = {'Session': lambda x: str(x)})\n",
    "sessionQuality['Electrode'] -= 1\n",
    "noted_sessions = sessionQuality['Session'].to_numpy()\n",
    "goodElecs = (sessionQuality['Quality'] == 'excellent') | (sessionQuality['Quality'] == 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc83fb0-e7b2-43e0-a756-7e64a8217302",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_props = {}\n",
    "\n",
    "session_array = []\n",
    "elec_array = []\n",
    "\n",
    "for sInd, session in enumerate(Sessions):\n",
    "\n",
    "    if session in noted_sessions:\n",
    "        quality_session = sessionQuality[sessionQuality['Session'] == session]\n",
    "        good_elecs = (quality_session['Quality'] == 'excellent') | (quality_session['Quality'] == 'good')\n",
    "\n",
    "        good_elecs = quality_session[goodElecs]['Electrode'].to_numpy()\n",
    "        num_good_elecs = good_elecs.shape[0]        \n",
    "        elec_props[session] = good_elecs \n",
    "\n",
    "        session_array.extend([session]*num_good_elecs)\n",
    "        elec_array.extend(good_elecs)\n",
    "    else:\n",
    "        elec_props[session] = np.empty(0, dtype=np.int64)\n",
    "\n",
    "erp_props = np.array([session_array, elec_array]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d920-19c0-438a-abaa-0d07d9185733",
   "metadata": {},
   "source": [
    "# ERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f094f-f3e0-40d8-a66c-e88a643e7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elec = erp_props.shape[0]\n",
    "erp_array = np.zeros((lfp_array.shape[0], num_elec, 2))\n",
    "\n",
    "bad_erps = []\n",
    "\n",
    "flat_thresh = 1200\n",
    "peak_window = np.arange(75, 125)\n",
    "\n",
    "for eInd in range(num_elec):\n",
    "\n",
    "    session, elec = erp_props[eInd]\n",
    "    elec = int(elec)\n",
    "    \n",
    "    session_elecs = elec_props[session]\n",
    "    \n",
    "    lfpSession = lfp_array.sel(session=session)\n",
    "    condSession = aggCndArray.sel(session=session)\n",
    "    behSession = aggBehArray.sel(session=session)\n",
    "    \n",
    "    pFB = behSession[:,-1] == 1\n",
    "    nFB = behSession[:,-1] == 0\n",
    "\n",
    "    pCond = (condSession == 3) | (condSession == 4)\n",
    "    aCond = (condSession == 1) | (condSession == 2)\n",
    "\n",
    "    presenceCond = pCond & nFB\n",
    "    absenceCond = aCond & nFB\n",
    "    condition_list = [presenceCond, absenceCond]\n",
    "    \n",
    "    for cInd in range(2):\n",
    "\n",
    "        condition_erp = np.nanmean(lfpSession[:, condition_list[cInd], elec],1)\n",
    "        \n",
    "        if (condition_erp[peak_window].max() > flat_thresh) & (condition_erp[peak_window].mean() > 100):  \n",
    "            erp_array[:, eInd, cInd] = condition_erp\n",
    "        else:\n",
    "            erp_array[:, eInd, cInd] = np.nan\n",
    "            bad_erps.append(condition_erp)\n",
    "\n",
    "baselineRange = np.round(0.02*sampling_rate).astype(int)\n",
    "\n",
    "erpCoordDict = {\"condition\":['Presence', 'Absence'], \"electrode\": [prop[0] + '_' + prop[1] for prop in erp_props]}\n",
    "erpAttributes = {'MaxLead': maxLead, 'MaxLag': maxLag, 'CenterEvent': centerEvent}\n",
    "\n",
    "erp_array = xr.DataArray(erp_array, dims = [\"time\", \"electrode\", \"condition\"], coords=erpCoordDict, attrs=erpAttributes)\n",
    "erp_array = erp_array.dropna('electrode')\n",
    "num_elec = erp_array.electrode.shape[0]\n",
    "\n",
    "erp_array.to_netcdf('../task_erps_all_nFB.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06786ccb-6943-4a50-b230-b76cf972e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_preprocess_dir = ''\n",
    "\n",
    "save_string = 'ERP/'\n",
    "\n",
    "erpAttrs = erp_array.attrs\n",
    "max_lead = erpAttrs['MaxLead']*1000\n",
    "max_lag = erpAttrs['MaxLag']*1000\n",
    "\n",
    "tend_erp = 100.0\n",
    "dt_erp=0.1\n",
    "t0_erp=0.0\n",
    "ts_erp = np.arange(t0_erp, tend_erp + dt_erp, dt_erp)\n",
    "nt = ts_erp.shape[0]\n",
    "\n",
    "eInd = 0\n",
    "emp_scale = int(lfp_array.mean('trial').max().round(-3)/1000)\n",
    "\n",
    "\n",
    "srOrig = sampling_rate\n",
    "srOrig = nt*srOrig/lfp_array.shape[0]\n",
    "lowpass = signal.butter(3, 20, 'lp', fs=srOrig, output='sos')\n",
    "\n",
    "tVec = np.linspace(0, ts_erp[-1], erp_array.shape[0])\n",
    "tVec_Extended = ts_erp\n",
    "\n",
    "task_ERPs = xr.DataArray(np.zeros((tVec_Extended.shape[0], erp_array.shape[1], 2)), dims=erp_array.dims, coords=erp_array.coords, attrs=erp_array.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d8bd8-2563-4aa5-bd37-73e34e0df395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for eInd in range(num_elec):\n",
    "    \n",
    "    newData = []\n",
    "\n",
    "    for cInd in range(2):\n",
    "        \n",
    "        data = erp_array[:, eInd, cInd].copy()\n",
    "\n",
    "        data = signal.sosfiltfilt(lowpass, data, axis = 0)\n",
    "        interpFunc = interpolate.interp1d(tVec, data, kind='cubic')\n",
    "        data = interpFunc(tVec_Extended)\n",
    "        newData.append(data)\n",
    "\n",
    "    newData = np.array(newData).T\n",
    "\n",
    "    dataMax = np.max(newData, axis=0)\n",
    "    dataMin = np.min(newData, axis=0)\n",
    "\n",
    "    dataDiffs = dataMax - dataMin\n",
    "    dataDiffs /= dataDiffs.max()\n",
    "\n",
    "    newData = preprocessing.MinMaxScaler().fit_transform(newData)\n",
    "    newData = newData * dataDiffs * emp_scale\n",
    "    newData -= newData[0,:]\n",
    "\n",
    "    newData = newData.T + np.random.randn(newData.shape[0])*0.0\n",
    "    newData = newData.T\n",
    "    \n",
    "    task_ERPs[:, eInd, :] = newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd4bcb-b61a-41c4-b5ba-d5b21d7949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colors = ['b', 'r']\n",
    "zords = [1,2]\n",
    "for eInd in range(num_elec):\n",
    "    for cInd in range(2):\n",
    "        ax.plot(task_ERPs.isel(electrode=eInd, condition=cInd), c=colors[cInd], alpha=0.5, zorder=zords[cInd]);\n",
    "fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10fcc0-d3ab-4a08-9e3e-cbb6a5c3f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_erps_df = task_ERPs.max('time').to_dataframe(name='value').reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f72fec-27c3-46f9-987e-a393a566d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ERPs_MinMax = task_ERPs.copy()\n",
    "\n",
    "for eInd in range(num_elec):\n",
    "    \n",
    "    newData = []\n",
    "\n",
    "    for cInd in range(2):\n",
    "        \n",
    "        data = erp_array[:, eInd, cInd].copy()\n",
    "    \n",
    "        data = signal.sosfiltfilt(lowpass, data, axis = 0)\n",
    "        interpFunc = interpolate.interp1d(tVec, data, kind='cubic')\n",
    "        data = interpFunc(tVec_Extended)\n",
    "        newData.append(data)\n",
    "\n",
    "    newData = np.array(newData).T\n",
    "\n",
    "    dataMax = np.max(newData, axis=0)\n",
    "    dataMin = np.min(newData, axis=0)\n",
    "\n",
    "    dataDiffs = dataMax - dataMin\n",
    "    dataDiffs /= dataDiffs.max()\n",
    "    \n",
    "    newData = preprocessing.MinMaxScaler().fit_transform(newData)\n",
    "    newData = newData * dataDiffs\n",
    "    \n",
    "    newData -= newData[0,:]\n",
    "\n",
    "    newData = newData.T + np.random.randn(newData.shape[0])*0.0\n",
    "    newData = newData.T\n",
    "    \n",
    "    task_ERPs_MinMax[:, eInd, :] = newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdd5e5-661d-419b-a327-6f3ccee005ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sData = task_ERPs\n",
    "sData = task_ERPs_MinMax\n",
    "\n",
    "sessERP_Peaks = np.max(sData, axis = 0)\n",
    "sessERP_Peaks = sessERP_Peaks/sessERP_Peaks.max(axis=0)\n",
    "\n",
    "pDomSessInds = np.where(sessERP_Peaks[:,0]>sessERP_Peaks[:,1])[0]\n",
    "aDomSessInds = np.where(sessERP_Peaks[:,0]<sessERP_Peaks[:,1])[0]\n",
    "\n",
    "pDomERPs = sData[:,pDomSessInds,:]\n",
    "aDomERPs = sData[:,aDomSessInds,:]\n",
    "\n",
    "task_ERPs.to_netcdf(parent_preprocess_dir + 'task_ERPs_AllRegions.nc')\n",
    "task_ERPs_MinMax.to_netcdf(parent_preprocess_dir + 'normalized_task_ERPs_AllRegions.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105531d1-3f98-42d4-9f8c-dc42ba85e220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9eb56-964b-4375-b311-47c239cdcd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
