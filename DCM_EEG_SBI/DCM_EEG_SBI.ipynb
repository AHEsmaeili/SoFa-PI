{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import inspect\n",
    "import glob\n",
    "from copy import copy, deepcopy\n",
    "from itertools import combinations\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as plticker\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from statannotations.Annotator import Annotator\n",
    "import xarray as xr\n",
    "# import cf_xarray as cfxr\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as scp\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from scipy import signal, stats\n",
    "from sklearn import preprocessing, decomposition\n",
    "\n",
    "import numba\n",
    "import numpyro as npr\n",
    "import numpyro.infer\n",
    "from numpyro import distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.integrate import solve_ivp, quad\n",
    "import arviz as az\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import torch\n",
    "import sbi \n",
    "import sbi.inference\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, SNLE, SNRE, prepare_for_sbi ,simulate_for_sbi\n",
    "from sbi.inference import likelihood_estimator_based_potential, DirectPosterior, MCMCPosterior, VIPosterior\n",
    "from sbi.analysis import ActiveSubspace, pairplot\n",
    "import sbi.utils as utils\n",
    "\n",
    "import mne\n",
    "import mne_connectivity\n",
    "mne.utils.set_config('MNE_USE_CUDA', 'true')\n",
    "mne.set_log_level('error')  # reduce extraneous MNE output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 1049\n",
    "np.random.seed(seed)\n",
    "cp.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "    \n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['FreeSans']})\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"axes.labelsize\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of functions for running linear regression / bootstrapping with jax\n",
    "def linear_function(x, a, b):\n",
    "    return (a*x + b)\n",
    "\n",
    "def linreg_system(N, y, x=None):\n",
    "    a = npr.sample('a', dist.Normal(0, 10))\n",
    "    b = npr.sample('b', dist.Normal(50, 100))\n",
    "    sigma= npr.sample('sigma', dist.HalfNormal(100))\n",
    "    xdot = npr.deterministic('xdot', linear_function(x=x, a=a, b=b))\n",
    "\n",
    "    with npr.plate('N', N):\n",
    "        npr.sample('obs', dist.Normal(xdot, sigma), obs=y)\n",
    "\n",
    "# Sampling method for MCMC sampling from a pre-defined system\n",
    "def run_mcmc_from_system(target_system, x, y, num_warmup=1000, num_samples=2000):\n",
    "\n",
    "    N = x.size\n",
    "\n",
    "    if type(x) == xr.core.dataarray.DataArray:\n",
    "        x = x.to_numpy()\n",
    "\n",
    "    nuts_kernel = NUTS(target_system, adapt_step_size=True)\n",
    "    mcmc = MCMC(nuts_kernel, num_chains=1, num_warmup=num_warmup, num_samples=num_samples)\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    mcmc.run(rng_key, N=N, y=y, x=x)\n",
    "\n",
    "    return mcmc # MCMC object is returned for storing sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Wasserstein distance between two distributions\n",
    "def wasserstein_distance(sample1, sample2, n_permutations=100):\n",
    "\n",
    "    combined_data = np.concatenate([sample1, sample2])\n",
    "    observed_distance = stats.wasserstein_distance(sample1, sample2)\n",
    "    \n",
    "    # Number of permutations\n",
    "    p_value = 0\n",
    "    for _ in range(n_permutations):\n",
    "      # Shuffle data points within combined sample\n",
    "      shuffled_data = np.random.permutation(combined_data)\n",
    "      shuffled_sample1 = shuffled_data[:len(sample1)]\n",
    "      shuffled_sample2 = shuffled_data[len(sample1):]\n",
    "      \n",
    "      # Calculate distance for shuffled samples\n",
    "      shuffled_distance = stats.wasserstein_distance(shuffled_sample1, shuffled_sample2)\n",
    "      \n",
    "      # Update p-value if shuffled distance is greater than observed\n",
    "      if shuffled_distance >= observed_distance:\n",
    "        p_value += 1\n",
    "    \n",
    "    p_value /= n_permutations\n",
    "\n",
    "    return (observed_distance, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing functional connectivity\n",
    "def compute_FC(data, fc_only=True):\n",
    "    \n",
    "    fc = np.corrcoef(data)\n",
    "    cov = np.cov(data)\n",
    "\n",
    "    if fc_only:\n",
    "        return fc\n",
    "    else:\n",
    "        return fc, cov\n",
    "\n",
    "# Calculate parameters for a simulation batch based on a parent params dictionary\n",
    "def get_batch_params(params, batch_size, batch_ind):\n",
    "    \n",
    "    batch_pars = deepcopy(params)\n",
    "    batch_pars['ns'] = batch_size\n",
    "    \n",
    "    for conn_key in ['C0', 'C1', 'C2', 'C3']:\n",
    "        batch_pars[conn_key] = params[conn_key][:, batch_size*batch_ind : batch_size*(batch_ind+1)]\n",
    "\n",
    "    return batch_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_axis_spines(ax, which=None, base=1.0, xticks=[], yticks=[], yaxis_left=True, xaxis_bot=True):\n",
    "\n",
    "    tick_locator = plticker.MultipleLocator(base=base)\n",
    "\n",
    "    if yaxis_left: \n",
    "        ax.spines.right.set(visible=False)\n",
    "        yspine = ax.spines.left\n",
    "    else:\n",
    "        ax.spines.left.set(visible=False)\n",
    "        yspine = ax.spines.right\n",
    "        \n",
    "    if xaxis_bot:\n",
    "        ax.spines.top.set(visible=False)\n",
    "        xspine = ax.spines.bottom\n",
    "    else:\n",
    "        ax.spines.bottom.set(visible=False)\n",
    "        xspine = ax.spines.top\n",
    "                           \n",
    "    if 'x' in which:\n",
    "        if len(xticks) == 0:\n",
    "            xticks = ax.get_xticks() \n",
    "            ax.xaxis.set_major_locator(tick_locator)\n",
    "        ax.set_xticks(xticks)\n",
    "        xspine.set_bounds(ax.get_xticks()[0], ax.get_xticks()[-1])\n",
    "        \n",
    "    else:\n",
    "        ax.spines.bottom.set(visible=False)\n",
    "    \n",
    "    if 'y' in which:\n",
    "        if len(yticks) == 0:\n",
    "            yticks = ax.get_yticks()\n",
    "        ax.set_yticks(yticks)\n",
    "        yspine.set_bounds(ax.get_yticks()[0], ax.get_yticks()[-1])\n",
    "        if len(yticks) == 0:\n",
    "            ax.yaxis.set_major_locator(tick_locator)\n",
    "    else:\n",
    "        ax.spines.left.set(visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './DCM_EEG_SBI/input/'    # Simulations go here\n",
    "output_dir = './DCM_EEG_SBI/output/'    # Features & learned posteriors go here\n",
    "\n",
    "parent_preprocess_dir = ''    # Directory for loading all preprocessed data\n",
    "measurements_dir = ''    # Directory for lateral-interception data\n",
    "\n",
    "subject_folders = glob.glob(measurements_dir + 'pongFac23*')\n",
    "subjects = np.array([subj.split('_')[-1] for subj in subject_folders])    # Load subject directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schaefer 2018 structural connectivity (SC) directories\n",
    "schaefer_sc_dir = ''\n",
    "SC_dirs = glob.glob(schaefer_sc_dir + '**/**/weights_Schaefer2018_400Parcels_7Networks_15M.txt', recursive=True)\n",
    "\n",
    "SC_avg_path = parent_preprocess_dir + 'Schaefer2018_SC.npy'\n",
    "\n",
    "if not os.path.isfile(SC_avg_path):\n",
    "    SC_array = np.array([np.loadtxt(scd) for scd in SC_dirs])    # Load all SC matrices\n",
    "    SC = SC_array.mean(0) \n",
    "    SC = np.log(SC+1) # Compute a normalized average SC to be used for all simulations\n",
    "    np.save(SC_avg_path, SC)\n",
    "else:\n",
    "    SC = np.load(SC_avg_path)\n",
    "\n",
    "num_node = len(SC)    # Total number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schaefer 2018 node names (ordered)\n",
    "schaefer_array = np.loadtxt(schaefer_sc_dir + 'Schaefer2018_400Parcels_7Networks_order_Main.txt', dtype=object)\n",
    "schaefer_labels = schaefer_array[:,1].copy()    # Take only the names\n",
    "\n",
    "# Schaefer 2018 -Yeo- functional network names\n",
    "yeo_networks = np.array(['DorsAttn', 'SalVentAttn', 'SomMot', 'Vis', 'Cont', 'Default', 'Limbic'])\n",
    "yeo_networks_shortened = ['DAN', 'VAN', 'SOM', 'VIS', 'FPN', 'DMN', 'LIM'][:len(yeo_networks)]\n",
    "\n",
    "# Get label names & indices\n",
    "network_label_names = np.array([l_name[13:].split('_')[0] for l_name in schaefer_labels], dtype=object)\n",
    "network_label_inds = {n_name: list(np.where(network_label_names == n_name)[0]) for n_name in yeo_networks}\n",
    "network_dimensions = {n_name: len(network_label_inds[n_name]) for n_name in yeo_networks}     # Network size for sum(FC) normalization\n",
    "network_node_list = list(network_label_inds.values())\n",
    "num_sim_networks = len(network_node_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_decim = 100    # Degree of sim. timeseries decimation\n",
    "num_sim = 20000\n",
    "\n",
    "# Jansen-Rit parameters across nodes/simulations - C1 (g2) values will be reassigned via samples from the prior distribution\n",
    "C0_base = 135.0\n",
    "C0 = 1 * C0_base * np.ones((num_node, num_sim))\n",
    "C1 = 0.8 * C0_base * np.ones((num_node, num_sim))\n",
    "C2 = 0.25 * C0_base * np.ones((num_node, num_sim))\n",
    "C3 = 0.25 * C0_base * np.ones((num_node, num_sim))\n",
    "\n",
    "# Prior generation\n",
    "prior_type = 'Informative'\n",
    "\n",
    "C1_min = 0.75*C0_base\n",
    "C1_max = 0.82*C0_base\n",
    "\n",
    "prior_min = [C1_min]*num_sim_networks\n",
    "prior_max = [C1_max]*num_sim_networks\n",
    "\n",
    "# Generating prior distribution based on input min/max\n",
    "prior_dist = utils.torchutils.BoxUniform(low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max))\n",
    "prior, _, _ = utils.user_input_checks.process_prior(prior_dist)\n",
    "theta = prior.sample((num_sim,))    # Sampling from the prior distribution\n",
    "\n",
    "for net_ind in range(num_sim_networks):\n",
    "    C1[network_node_list[net_ind],:] = np.array(theta[:,net_ind])    # Assigning g2 values from sampled parameters\n",
    "\n",
    "# All chosen parameters & inputs must be defined in a simulation dictionary \n",
    "params = {\"SC\": SC,\n",
    "          \"ns\": num_sim,\n",
    "          \"dt\": 0.05,\n",
    "          \"decimate\":sim_decim,\n",
    "          \"engine\": \"gpu\",\n",
    "          \"C0\": C0,\n",
    "          \"C1\": C1, \n",
    "          \"C2\": C2, \n",
    "          \"C3\": C3,\n",
    "          \"t_end\": 3000, \n",
    "          \"t_cut\": 2000,\n",
    "          \"integration_method\": \"heun\",\n",
    "          \"mu\": 0.295,\n",
    "          \"sigma\": 0.0}\n",
    "\n",
    "sim_pars_sbi = deepcopy(params)    # Original param. dict. is kept for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining simulation name and creating directories\n",
    "sim_name = 'JR_SDE_SBI_C1_' + str(num_sim_networks) + 'Networks_' + prior_type + '_'\n",
    "\n",
    "sbi_path = input_dir + sim_name + str(num_sim)\n",
    "posterior_path = output_dir + sim_name + str(num_sim)\n",
    "\n",
    "overwrite = False\n",
    "\n",
    "if not os.path.exists(sbi_path):\n",
    "    os.mkdir(sbi_path)\n",
    "\n",
    "if not os.path.exists(posterior_path):\n",
    "    os.mkdir(posterior_path)\n",
    "\n",
    "# Saving simulation dictionary, sampled parameters and the parameter distribution object\n",
    "with open(os.path.join(sbi_path, 'default_parameters_sbi.pkl'), 'wb') as f:\n",
    "    pickle.dump(sim_pars_sbi, f)\n",
    "\n",
    "if overwrite:\n",
    "    torch.save(theta, f=sbi_path + '/theta.pt')\n",
    "    torch.save(prior_dist, f=sbi_path + '/prior.pt')\n",
    "\n",
    "theta = torch.load(sbi_path+'/theta.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching simulations due to large simulation size\n",
    "num_batches = 5\n",
    "batch_size = int(num_sim/num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running batch simulations\n",
    "for batch_ind in range(num_batches):\n",
    "    \n",
    "    batch_params = get_batch_params(sim_pars_sbi, batch_size, batch_ind)\n",
    "\n",
    "    sol = JR(batch_params)    # Preparing the simulation object with the batch param. dict. \n",
    "    data = sol.simulate()     # Running simulation batch\n",
    "\n",
    "    np.savez(sbi_path + '/simulations_' + str(batch_ind) + '.npz', x=data['x'], t=data['t'], theta=theta)\n",
    "\n",
    "    if batch_ind == 0:\n",
    "\n",
    "        sim_array = data['x']\n",
    "        time_array = data['t']\n",
    "        sim_samples = xr.DataArray(sim_array, dims=('time', 'label', 'simulation'), coords={'label':schaefer_array[:,1], 'time': sim_time_sbi})\n",
    "        sim_samples.to_netcdf(sbi_eeg_path + '/simulations_sample.nc')    # Save a sample of simulations for plotting with label names\n",
    "        del(sim_samples)\n",
    "    \n",
    "    else:\n",
    "        sim_array = np.concatenate((sim_array, data['x']), axis = -1)\n",
    "    \n",
    "    del(data, sol); gc.collect()    # Delete residual data from memory before the next batch\n",
    "\n",
    "# Saving concatenated simulations for convenience\n",
    "sim_dims = ('time', 'label', 'simulation')\n",
    "sim_coords = {'label': schaefer_labels}\n",
    "sim_array = xr.DataArray(sim_array, dims=sim_dims, coords=sim_coords)\n",
    "sim_array.to_netcdf(sbi_path + '/simulations.nc')\n",
    "del(sim_array); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing functional connectivity from batch simulations\n",
    "sim_dims = ('time', 'label', 'simulation')\n",
    "sim_coords = {'label': schaefer_labels}\n",
    "\n",
    "for batch_ind in range(num_batches):\n",
    "\n",
    "    fc_batch = xr.DataArray(np.zeros((num_node, num_node, batch_size), dtype=np.float32), dims = ('label_1', 'label_2', 'simulation'),\n",
    "                                coords = {'label_1': network_label_names, 'label_2': network_label_names})\n",
    "    \n",
    "    sim_batch = xr.DataArray(np.load(sbi_path + '/simulations_' + str(batch_ind) + '.npz')['x'], dims=sim_dims, coords=sim_coords)   \n",
    "    sim_batch = sim_batch/sim_batch.max(('time','label'))\n",
    "    \n",
    "    for sim_ind in range(batch_size):\n",
    "        if sim_ind%batch_size==0:\n",
    "            print((batch_ind, sim_ind))\n",
    "        t_fc = compute_FC(sim_batch[:, :, sim_ind].T, fc_only=True)\n",
    "        fc_batch[:, :, sim_ind] = t_fc\n",
    "    \n",
    "    if batch_ind == 0:\n",
    "        fc_sim_array = fc_batch.copy()        \n",
    "    else:\n",
    "        fc_sim_array = xr.concat((fc_sim_array, fc_batch), 'simulation')\n",
    "    \n",
    "    del(sim_batch, fc_batch);gc.collect()\n",
    "\n",
    "fc_sim_array.to_netcdf(sbi_path + '/fc_sim_array.nc')    # Concatenating/saving all FC matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating upper triangular indices for each network\n",
    "triu_networks = {net_name: np.tril(np.ones((num_nnodes, num_nnodes), dtype=bool), k=0) for net_name, num_nnodes in network_dimensions.items() if net_name != 'Global'}\n",
    "network_sum_count = {net_name: (net_dim**2 - net_dim)/2 for net_name, net_dim in network_dimensions.items()}    # For sum(FC) norm.\n",
    "global_sum_count = {net_name: len(network_integration_inds[net_name])**2 for net_name in yeo_networks}\n",
    "\n",
    "num_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation/FC arrays can be loaded instead of running the majority of previous steps\n",
    "sim_array = xr.load_dataarray(sbi_path + '/simulations.nc')\n",
    "fc_sim_array = xr.open_dataarray(sbi_path + '/fc_sim_array.nc')\n",
    "\n",
    "sim_array = sim_array/sim_array.max(('time','label'))\n",
    "num_timepoints = len(sim_array.time)\n",
    "num_sim = len(sim_array.simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Sum(FC) per network\n",
    "num_sim = len(fc_sim_array.simulation)\n",
    "\n",
    "fc_sum_sim = xr.DataArray(np.zeros((num_sim_networks, num_sim, 1), dtype=np.float32), dims = ('network', 'simulation', 'summary'), coords = {'network': yeo_networks, 'summary': ['sum']})\n",
    "\n",
    "for net_ind, net_name in enumerate(yeo_networks):\n",
    "\n",
    "    print(net_name)\n",
    "    \n",
    "    sel_fc = fc_sim_array.sel(label_1=net_name, label_2=net_name).load().transpose('simulation', 'label_1', 'label_2').to_numpy()\n",
    "        \n",
    "    tril_label_mask = triu_networks[net_name]    \n",
    "    sel_fc[..., tril_label_mask] = 0\n",
    "\n",
    "    fc_sum_sim[net_ind, :, :] = sel_fc.sum((-2, -1)).reshape(-1,1)/network_sum_count[net_name]\n",
    "    \n",
    "    del(sel_fc, integration_sims, integration_sums);gc.collect()\n",
    "\n",
    "fc_sum_sim.to_netcdf(sbi_path + '/fc_sum_sim.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_sum_sim = xr.load_dataarray(sbi_path + '/fc_sum_sim.nc')\n",
    "fc_eig_sim = xr.load_dataarray(sbi_path + '/fc_eig_sim.nc')\n",
    "\n",
    "feature_order =('network', 'summary')\n",
    "sel_feature_names = ['sum']\n",
    "\n",
    "fc_summary_sim = xr.concat((fc_sum_sim, fc_eig_sim), dim='summary')\n",
    "\n",
    "# Using only Sum(FC) for training\n",
    "x_feature_array = fc_summary_sim.sel(summary=sel_feature_names)\n",
    "x_feature_array.coords['network'] = yeo_networks_shortened\n",
    "x_feature_array = x_feature_array.stack({'feature': feature_order}).squeeze()\n",
    "\n",
    "num_features = len(x_feature_array.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False\n",
    "  \n",
    "x_features = torch.tensor(x_feature_array.to_numpy(), dtype=torch.float32)\n",
    "x_features.shape, theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parcellation type and location\n",
    "parc = 'Schaefer2018_400Parcels_7Networks_order'\n",
    "fs_label_dir = ''\n",
    "network_label_dict = {n_name: np.array([label.name for label in mne.read_labels_from_annot('fsaverage', parc=parc, regexp=n_name, subjects_dir=fs_label_dir)], dtype=object)\n",
    "                      for n_name in yeo_networks}\n",
    "\n",
    "# Load empirical FC arrays if not previously done\n",
    "fc_array_emp = xr.load_dataarray(parent_preprocess_dir + 'source_fc_7Networks.nc').load()\n",
    "fc_array_emp_global = xr.load_dataarray(parent_preprocess_dir + 'source_fc_global.nc')\n",
    "fc_array_emp = fc_array_emp.sel(network=yeo_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute task parameters\n",
    "task_conditions = fc_array_emp.condition.to_numpy()\n",
    "emp_sources = fc_array_emp.source.to_numpy()\n",
    "network_dimensions = fc_array_emp.attrs\n",
    "emp_network_names = fc_array_emp.network\n",
    "\n",
    "num_subjects = len(fc_array_emp.subject)\n",
    "num_emp_sources = len(emp_sources)\n",
    "num_emp_networks = len(fc_array_emp.network)\n",
    "num_emp_labels = len(fc_array_emp.label_1)\n",
    "num_conditions = len(task_conditions)h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute upper triangular matrices from empirical \n",
    "tril_label_mask = np.tri(num_emp_labels, num_emp_labels, dtype=bool)\n",
    "\n",
    "fc_emp_upper = fc_array_emp.copy().to_numpy()\n",
    "fc_emp_upper[:, :, :, :, tril_label_mask] = 0\n",
    "fc_emp_upper = xr.DataArray(fc_emp_upper, dims = fc_array_emp.dims, coords = fc_array_emp.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fc_sum_dims = ('subject', 'condition', 'source', 'network', 'summary')\n",
    "fc_sum_coords = {'subject':subjects, 'condition': task_conditions, 'source': emp_sources, 'network': emp_network_names, 'summary':['sum']}\n",
    "fc_eig_coords = deepcopy(fc_sum_coords)\n",
    "fc_eig_coords['summary'] = ['eig_' + str(ind+1) for ind in range(num_components)]\n",
    "\n",
    "# Initializing emp. feature arrays \n",
    "fc_sum_emp_netnorm = xr.DataArray(np.zeros((num_subjects, num_conditions, num_emp_sources, num_emp_networks, 1)), dims = fc_sum_dims, coords = fc_sum_coords)\n",
    "\n",
    "# The eigenvalues can be used to compare their suitability as a features vs. Sum(FC) \n",
    "fc_eig_emp = xr.DataArray(np.zeros((num_subjects, num_conditions, num_emp_sources, num_emp_networks, num_components)), dims = fc_sum_dims, coords = fc_eig_coords)\n",
    "\n",
    "for net_ind, net_name in enumerate(yeo_networks):\n",
    "\n",
    "    network_dim = network_dimensions[net_name]\n",
    "\n",
    "    sel_fc = fc_emp_upper[..., net_ind, :network_dim, :network_dim]\n",
    "\n",
    "    fc_sum_emp_netnorm[..., net_ind, :] = sel_fc.sum(('label_1', 'label_2')).to_numpy()[..., None]/network_sum_count[net_name]\n",
    "    \n",
    "    sel_fc = fc_array_emp[..., net_ind, :network_dim, :network_dim]\n",
    "\n",
    "    fc_eigs, _ = np.linalg.eig(sel_fc)\n",
    "    fc_eig_emp[..., net_ind, :] = np.abs(fc_eigs)[..., :num_components]\n",
    "\n",
    "fc_sum_emp_netnorm.to_netcdf(parent_preprocess_dir + 'fc_sum_emp_netnorm_7Networks_norm_full.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all features\n",
    "fc_summary_emp = xr.concat((fc_sum_emp_netnorm, fc_eig_emp), dim='summary')\n",
    "\n",
    "emp_feature_array = fc_summary_emp.sel(summary=sel_feature_names)    # Pick from the chosen feature names only\n",
    "emp_feature_array.coords['network'] = yeo_networks_shortened\n",
    "emp_feature_array = emp_feature_array.stack({'feature': feature_order}).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the source of empirical timeseries and normalize -if the same has been conducted in syn. data-\n",
    "sel_source = 'epochs'\n",
    "emp_feature_array = emp_feature_array.sel(source=sel_source)\n",
    "\n",
    "if normalize:\n",
    "    emp_feature_array /= emp_feature_array.max()\n",
    "\n",
    "emp_features = emp_feature_array.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = np.unique(emp_features.summary.to_numpy())\n",
    "\n",
    "estimator_type = 'nsf'    # Neural spline flow\n",
    "\n",
    "saveString = 'features_'+ '-'.join(feature_list) + '_' + estimator_type    # The subtype of posterior estimator\n",
    "print(saveString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Training the neural density estimator based on input features & parameters\n",
    "inference = SNPE(prior, density_estimator=estimator_type, device='cpu')\n",
    "posterior_estimator = inference.append_simulations(theta, x_features).train()\n",
    "\n",
    "print (\"-\"*60)\n",
    "print(\"---training took:  %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "with open(posterior_path + '/' + saveString + '.pkl', 'wb') as f:\n",
    "    pickle.dump(posterior_estimator,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learned weights can be saved, bypassing the need for retraining\n",
    "posterior_file = open(posterior_path + '/' + saveString + '.pkl', \"rb\")\n",
    "posterior_estimator = pickle.load(posterior_file)\n",
    "posterior_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a random value from the parameter set\n",
    "theta_true_ind = np.random.choice(np.arange(theta.shape[0]))\n",
    "theta_true_sample = theta[theta_true_ind, :]\n",
    "\n",
    "theta_true_feat = x_features[theta_true_ind,:]    # Corresponding feature value(s) of the sampled (observed) parameter\n",
    "theta_true_ind, theta_true_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=10000    # Number of samples per inferred data point\n",
    "posterior = DirectPosterior(posterior_estimator, prior)    # The prior distribution is necessary for creating the posterior\n",
    "\n",
    "theta_pred = posterior.sample((num_samples,), theta_true_feat).numpy().squeeze()    \n",
    "theta_pred_max = np.mean(theta_pred,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,7, figsize=(12,3), sharey=True)\n",
    "\n",
    "truth_color = 'k'\n",
    "pred_color = 'coral'\n",
    "bw_adjust=3\n",
    "\n",
    "for ax_ind, ax in enumerate(axes.ravel()):\n",
    "\n",
    "    sns.kdeplot(theta[:, ax_ind].numpy().ravel(), ax=ax, shade=True, linewidth=0, color='slategray', alpha=0.6, label='Prior', cut=0, bw_adjust=1)\n",
    "    sns.kdeplot(theta_pred[:, ax_ind], ax=ax, shade=True, linewidth=0, color='steelblue', alpha=0.6, label='Posterior', cut=0, bw_adjust=bw_adjust)\n",
    "    ax.axvline(theta_true_sample.numpy()[ax_ind], ymin=0, c=truth_color, ls='-', label='Observed', lw=2)\n",
    "    ax.axvline(theta_pred_max[ax_ind], ymin=0, c=pred_color, ls='--', label='Predicted', lw=2)\n",
    "    ax.set_title(yeo_networks_shortened[ax_ind], fontsize=10, pad=10)\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    if ax_ind == 0:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    modify_axis_spines(ax, which=['x','y'], xticks=[101, 111], yticks=[0, 0.4])\n",
    "\n",
    "fig.supylabel('Density',x=0.08, fontsize=10)\n",
    "fig.supxlabel('$g_2$', y=-0.02, fontsize=10)\n",
    "\n",
    "fig.legend(handles, labels, frameon=False, loc=(0.89, 0.65), ncols=1)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "fig.savefig(fig_save_loc + '/SBI_validation_posteriors_eeg.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the observed and predicted parameters to simulate from the generative model  \n",
    "theta_check = np.concatenate((theta_true_sample[None, ...], theta_pred_max[None, ...]),0)\n",
    "theta_check.shape\n",
    "\n",
    "sim_decim = 100\n",
    "num_check_sim = 2\n",
    "\n",
    "C0_base = 135.0\n",
    "C0_check = 1 * C0_base * np.ones((num_node, num_check_sim))\n",
    "C1_check = 0.8 * C0_base * np.ones((num_node, num_check_sim))\n",
    "C2_check = 0.25 * C0_base * np.ones((num_node, num_check_sim))\n",
    "C3_check = 0.25 * C0_base * np.ones((num_node, num_check_sim))\n",
    "\n",
    "for net_ind in range(num_sim_networks):\n",
    "    C1_check[network_node_list[net_ind],:] = np.array(theta_check[:,net_ind])\n",
    "\n",
    "check_params = {\"SC\": SC,\n",
    "          \"ns\": num_check_sim,\n",
    "          \"dt\": 0.05,\n",
    "          \"decimate\":sim_decim,\n",
    "          \"engine\": \"cpu\",\n",
    "          \"C0\": C0_check,\n",
    "          \"C1\": C1_check, \n",
    "          \"C2\": C2_check, \n",
    "          \"C3\": C3_check,\n",
    "          \"t_end\": 3000, \n",
    "          \"t_cut\": 2000,\n",
    "          \"integration_method\": \"heun\",\n",
    "          \"mu\": 0.295,\n",
    "          \"sigma\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sol = JR(check_params)\n",
    "check_data = check_sol.simulate()    # Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FC for predicted and observed parameters\n",
    "check_simulations = check_data['x']\n",
    "\n",
    "theta_true_eeg = check_simulations[:,:,0]\n",
    "theta_pred_eeg = check_simulations[:,:,1]\n",
    "\n",
    "fc_true_networks = {}\n",
    "fc_pred_networks = {}\n",
    "\n",
    "for net_name, net_inds in network_label_inds.items():\n",
    "    \n",
    "    fc_true_networks[net_name] = compute_FC(theta_true_eeg[:, net_inds].T)\n",
    "    fc_pred_networks[net_name] = compute_FC(theta_pred_eeg[:, net_inds].T)\n",
    "\n",
    "fc_true = compute_FC(theta_true_eeg.T)\n",
    "fc_pred = compute_FC(theta_pred_eeg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot observed/predicted FC matrices and integration values\n",
    "fig, axes = plt.subplots(2,7, figsize=(12,10))\n",
    "\n",
    "cmap='plasma'\n",
    "\n",
    "for ax_ind, (net_name, net_inds) in enumerate(network_label_inds.items()):\n",
    "\n",
    "    net_true_fc = np.triu(fc_true_networks[net_name])\n",
    "    net_pred_fc = np.triu(fc_pred_networks[net_name])\n",
    "\n",
    "    tril_indices = np.tril_indices_from(net_true_fc)\n",
    "    net_true_fc[tril_indices] = np.nan\n",
    "    net_pred_fc[tril_indices] = np.nan\n",
    "    \n",
    "    net_size = network_dimensions[net_name]\n",
    "    text_xloc = net_size*0.05\n",
    "    text_yloc = net_size*0.9\n",
    "    \n",
    "    if ax_ind == 0:\n",
    "        img_ax = axes[0, ax_ind].imshow(net_true_fc, cmap=cmap,vmin=-1)\n",
    "    else:\n",
    "        axes[0, ax_ind].imshow(net_true_fc, cmap=cmap,vmin=-1)\n",
    "    axes[1, ax_ind].imshow(net_pred_fc, cmap=cmap,vmin=-1)\n",
    "\n",
    "    axes[0, ax_ind].text(s='$Int. = {}$'.format(np.nansum(net_true_fc).round(-1)), x=text_xloc, y=text_yloc, fontsize=10)\n",
    "    axes[1, ax_ind].text(s='$Int. = {}$'.format(np.nansum(net_pred_fc).round(-1)), x=text_xloc, y=text_yloc, )\n",
    "\n",
    "    axes[0, ax_ind].set_xlabel(yeo_networks_shortened[ax_ind])\n",
    "    axes[0, ax_ind].xaxis.set_label_position('top')\n",
    "\n",
    "cbar = fig.colorbar(img_ax, cax=fig.add_axes([0.92, 0.375, 0.02, 0.241]))\n",
    "cbar.set_label('FC value')\n",
    "cbar.set_ticks([-1, 1])\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "axes[0,0].set_ylabel('Observed')\n",
    "axes[1,0].set_ylabel('Predicted')\n",
    "\n",
    "fig.subplots_adjust(hspace=-.8)\n",
    "\n",
    "fig.savefig(fig_save_loc + '/EEG/SBI_validation_FCs.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior sampling of empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior samples and \n",
    "num_samples=2000\n",
    "posterior = DirectPosterior(posterior_estimator, prior,)\n",
    "\n",
    "emp_posterior = xr.DataArray(np.zeros((num_subjects, num_conditions, num_emp_networks, num_samples)), dims = ('subject', 'condition', 'network', 'sample'),\n",
    "                             coords = {'subject': subjects, 'condition': task_conditions, 'network': emp_network_names})\n",
    "posterior_means = xr.DataArray(np.zeros((num_subjects, num_conditions, num_emp_networks)), dims=('subject', 'condition', 'network'),\n",
    "                              coords={'subject': subjects, 'condition': task_conditions, 'network': emp_network_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sInd, subject in enumerate(subjects):\n",
    "    \n",
    "    for cInd, condition in enumerate(task_conditions):\n",
    "\n",
    "        # Sample from the posterior for each emp. data point (i.e. condition/subject)\n",
    "        theta_posterior = posterior.sample((num_samples,), emp_features.sel(subject = subject, condition = condition).to_numpy(), show_progress_bars=True).numpy()\n",
    "        emp_posterior[sInd, cInd, :, :] = theta_posterior.T\n",
    "        posterior_means[sInd, cInd, :] = theta_posterior.mean(0)    # Save the mean of the posterior for correlation with behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of posterior sample means between 0 and 1 for better visualization across groups/conditions\n",
    "posterior_means_flat = preprocessing.MinMaxScaler().fit_transform(posterior_means.to_numpy().ravel().reshape(-1,1))\n",
    "posterior_means_norm = posterior_means_flat.reshape(posterior_means.shape)\n",
    "posterior_means_norm = xr.DataArray(posterior_means_norm, dims=posterior_means.dims, coords=posterior_means.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inferred samples and their respective means\n",
    "emp_posterior.to_netcdf(posterior_path + '/JR_7Networks_empirical_posterior.nc')\n",
    "posterior_means.to_netcdf(posterior_path + '/JR_7Networks_empirical_posterior_means.nc')\n",
    "\n",
    "posterior_means_norm.coords['subject'] = gen_list\n",
    "posterior_means_norm = posterior_means_norm.rename(subject='group')\n",
    "posterior_means_norm.to_netcdf(posterior_path + '/JR_7Networks_empirical_posterior_means_norm.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previous sampled posteriors if skipping previous steps\n",
    "emp_posterior = xr.load_dataarray(posterior_path + 'JR_7Networks_empirical_posterior.nc')\n",
    "posterior_means = xr.load_dataarray(posterior_path + 'JR_7Networks_empirical_posterior_means.nc')\n",
    "\n",
    "# Plotting parameters\n",
    "absence_color, presence_color = 'crimson', 'dodgerblue'\n",
    "tick_size = 16\n",
    "label_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string = 'EEG/'\n",
    "\n",
    "event_label = 'threshTime'\n",
    "\n",
    "mapped_dict = {-1: 'n', 1: 'p', 0.1: 'a-p', 1.0: 'p-a', 1: 'Male', 2: 'Female'}\n",
    " \n",
    "pong_results = xr.load_dataarray(parent_preprocess_dir + 'agg_pong_results.nc').load()\n",
    "pong_movement_raw = xr.load_dataarray(parent_preprocess_dir + 'agg_pong_movement_' + event_label + '_lock.nc').load()\n",
    "\n",
    "f_order = 2\n",
    "low_cut = 12\n",
    "lowpass = signal.butter(f_order, low_cut, fs = 120, btype = 'lp', output = 'sos') \n",
    "pong_movement_data = signal.sosfiltfilt(lowpass, pong_movement_raw.sel(source='movement'), axis = 0)\n",
    "pong_movement = pong_movement_raw.copy()\n",
    "pong_movement[:, :, 0, :] = pong_movement_data\n",
    "\n",
    "conditions = pong_results.sel(variable = 'cond')\n",
    "intercepts = pong_results.sel(variable = 'result')\n",
    "\n",
    "pcond = conditions == 1\n",
    "acond = conditions == 0\n",
    "\n",
    "negfb = intercepts == -1\n",
    "posfb = intercepts == 1\n",
    "\n",
    "subject_gens = pong_results.sel(variable = 'gender', trial = 0).to_numpy()\n",
    "gen_list = [mapped_dict[gen] for gen in subject_gens]\n",
    "\n",
    "num_subjects = len(subjects)\n",
    "male_subjects = subject_gens == 1\n",
    "female_subjects = subject_gens == 2\n",
    "\n",
    "subject_groups = [female_subjects, male_subjects]\n",
    "subject_group_names = ['Female', 'Male']\n",
    "num_subject_groups = len(subject_groups)\n",
    "subject_group_dict = {subject_group_names[ind]: subject_groups[ind] for ind in range(num_subject_groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_beh = np.zeros(len(subjects))\n",
    "a_beh = np.zeros(len(subjects))\n",
    "\n",
    "for sInd, subj in enumerate(subjects):\n",
    "    \n",
    "    if sInd == 0:\n",
    "        labels = ['Presence', 'Absence']\n",
    "    else:\n",
    "        labels = ['', '']\n",
    "    \n",
    "    sub_bs = pong_results.sel(subject = subj, variable = 'ms')\n",
    "    sub_bap = pong_results.sel(subject = subj, variable = 'BAP_new')\n",
    "    sub_bdp = pong_results.sel(subject = subj, variable = 'BDP_new')\n",
    "    sub_bap[sub_bap == 0] = 1\n",
    "        \n",
    "    sub_movement = np.abs(pong_movement.sel(subject = subj, source = 'movement')/sub_bap)\n",
    "    \n",
    "    sub_speed = np.gradient(sub_movement, axis = 0)\n",
    "    sub_speed = xr.DataArray(sub_speed, coords = sub_movement.coords, dims = sub_movement.dims)\n",
    "\n",
    "    stable_trials = ~(sub_movement[0,:] >= sub_movement[-1,:])    \n",
    "\n",
    "    nfb = negfb.sel(subject = subj)\n",
    "    pfb = posfb.sel(subject = subj)\n",
    "    \n",
    "    pres = pcond.sel(subject = subj)\n",
    "    abse = acond.sel(subject = subj)\n",
    "\n",
    "    p_tr = pres & stable_trials\n",
    "    a_tr = abse & stable_trials\n",
    "        \n",
    "    p_sum = (p_tr & pfb).sum('trial')\n",
    "    a_sum = (p_tr & pfb).sum('trial')\n",
    "\n",
    "    vel_metric = sub_speed\n",
    "    \n",
    "    p_met = vel_metric.sel(trial = p_tr).mean('time').mean('trial')\n",
    "    a_met = vel_metric.sel(trial = a_tr).mean('time').mean('trial')\n",
    "   \n",
    "    p_beh[sInd] = p_met\n",
    "    a_beh[sInd] = a_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_norm_concat = preprocessing.MinMaxScaler().fit_transform(np.concatenate((p_beh, a_beh)).reshape(-1,1)).ravel()\n",
    "p_beh = beh_norm_concat[:num_subjects]\n",
    "a_beh = beh_norm_concat[num_subjects:]\n",
    "\n",
    "cdiff_method = 'percentage'\n",
    "beh_ratio = p_beh/(p_beh+a_beh) * 100\n",
    "\n",
    "beh_ratio_groups = np.zeros((2,14))\n",
    "beh_ratio_groups[0,:] = beh_ratio[female_subjects]\n",
    "beh_ratio_groups[1,:male_subjects.sum()] = beh_ratio[male_subjects]\n",
    "beh_ratio_array = xr.DataArray(beh_ratio_groups, dims = ('group', 'subject'), coords = {'group': ['Female', 'Male']})\n",
    "\n",
    "pong_beh_df = pd.DataFrame({'Presence': p_beh, 'Absence': a_beh, 'group' : gen_list}).melt(id_vars = 'group', value_vars = ['Presence', 'Absence'], var_name = 'cond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Normalized Sum(FC) per subject group\n",
    "fc_sum_groups = fc_sum_emp_netnorm.sel(source='epochs').squeeze().rename(subject='group')\n",
    "\n",
    "fc_sum_groups.coords['group'] = gen_list\n",
    "fc_sum_groups.coords['network'] = yeo_networks_shortened\n",
    "fc_sum_groups = fc_sum_groups.groupby('group').mean()\n",
    "\n",
    "fc_sum_groups -= fc_sum_groups.min()\n",
    "fc_sum_groups /= fc_sum_groups.max()\n",
    "\n",
    "fc_sum_df = fc_sum_groups.reset_coords(names=['summary', 'source'], drop=True).to_dataframe(name='value').reset_index()\n",
    "fc_sum_df_reindexed = fc_sum_df.set_index('group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preallocating stats arrays\n",
    "fc_stats_sbi = np.zeros((num_subject_groups, num_emp_networks, 2))\n",
    "fc_stats_sbi = xr.DataArray(fc_stats_sbi, dims = ('group', 'network', 'stat'), coords = {'group': subject_group_names, 'network':posterior_means.network , 'stat': ['rvalue', 'pvalue']})\n",
    "fc_stats_nl_sbi = fc_stats_sbi.copy()\n",
    "\n",
    "# Between-condition ratio of neural data\n",
    "neu_ratio_sbi = xr.DataArray(np.zeros((num_subject_groups, num_emp_networks, 14)), dims = ('group', 'network', 'condition_ratio'), coords = {'group': subject_group_names, 'network': posterior_means.network})\n",
    "fc_linreg_sbi = xr.DataArray(np.zeros((num_subject_groups, num_emp_networks, 2, 14)), dims = ('group', 'network', 'bound', 'condition_ratio'), coords = {'group': subject_group_names, 'network': posterior_means.network,'bound': ['l_bound', 'u_bound']})\n",
    "fc_nonlinreg_sbi = fc_linreg_sbi.copy()\n",
    "\n",
    "num_warmup = 1000\n",
    "num_samples = 2000\n",
    "\n",
    "for gInd, group_name in enumerate(subject_group_names):\n",
    "\n",
    "    subject_group = subject_group_dict[group_name]\n",
    "    \n",
    "    b_ratio = beh_ratio_array.sel(group = group_name)\n",
    "    b_ratio = b_ratio[:subject_group.sum()].to_numpy()\n",
    "\n",
    "    for net_ind, network in enumerate(posterior_means.network):\n",
    "        \n",
    "        p_neu = posterior_means_norm.sel(condition = 'Presence', network=network)\n",
    "        a_neu = posterior_means_norm.sel(condition = 'Absence', network=network)\n",
    "\n",
    "        if cdiff_method == 'percentage':\n",
    "            n_ratio = p_neu/(p_neu+a_neu) * 100\n",
    "        else:\n",
    "            n_ratio = p_neu-a_neu\n",
    "\n",
    "        n_ratio = n_ratio[subject_group]\n",
    "\n",
    "        stat_res_linreg = stats.linregress(n_ratio, b_ratio)\n",
    "        \n",
    "        stat_res_con = stats.pearsonr(n_ratio, b_ratio)\n",
    "        stat_res_fc_nl = stats.spearmanr(n_ratio, b_ratio)\n",
    "        \n",
    "        fc_stats_sbi[gInd, net_ind, 0] = stat_res_con.statistic\n",
    "        fc_stats_sbi[gInd, net_ind, 1] = stat_res_con.pvalue\n",
    "    \n",
    "        fc_stats_nl_sbi[gInd, net_ind, 0] = stat_res_fc_nl.correlation\n",
    "        fc_stats_nl_sbi[gInd, net_ind, 1] = stat_res_fc_nl.pvalue\n",
    "    \n",
    "        x_mcmc = np.sort(n_ratio)\n",
    "        x_sorted_inds = np.argsort(n_ratio)\n",
    "        y_mcmc = b_ratio[x_sorted_inds]\n",
    "        \n",
    "        mcmc_linear = run_mcmc_from_system(linreg_system, x=x_mcmc, y=y_mcmc, num_warmup = num_warmup, num_samples = num_samples)\n",
    "        samples_linear = az.from_numpyro(mcmc_linear)\n",
    "        xdot_quantiles_linear = np.quantile(samples_linear.posterior.xdot.squeeze(),[0.05,0.95],axis=0)\n",
    "        \n",
    "        fc_linreg_sbi[gInd, net_ind, :, :subject_group.sum()] = xdot_quantiles_linear    \n",
    "        neu_ratio_sbi[gInd, net_ind, :subject_group.sum()] = n_ratio.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save computed statistics\n",
    "fc_stats_sbi.to_netcdf(parent_preprocess_dir + '/fc_stats_sbi.nc')\n",
    "fc_stats_nl_sbi.to_netcdf(parent_preprocess_dir + '/fc_stats_nl_sbi.nc')\n",
    "fc_linreg_sbi.to_netcdf(parent_preprocess_dir + '/fc_linreg_sbi.nc')\n",
    "neu_ratio_sbi.to_netcdf(parent_preprocess_dir + '/neu_ratio_sbi.nc')\n",
    "\n",
    "# Save posterior samples for each subject group\n",
    "group_posteriors = emp_posterior.copy()\n",
    "group_posteriors.coords['network'] = yeo_networks_shortened\n",
    "\n",
    "group_posteriors['subject'] = gen_list\n",
    "group_posteriors = group_posteriors.rename({'subject':'group'})\n",
    "group_posteriors.to_netcdf(parent_preprocess_dir + '/group_posteriors.nc')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
